version: '3.8'

services:
  # Eureka сервер
  em-discovery:
    build:
      context: ../em-discovery
      dockerfile: Dockerfile
    env_file:
      - ./env/discovery.prod.env
    ports:
      - "8761:8761"
    volumes:
      - ./logs:/app/logs
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081/actuator/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # API Gateway
  em-gateway:
    build:
      context: ../em-gateway
      dockerfile: Dockerfile
    env_file:
      - ./env/gateway.prod.env
    ports:
      - "8080:8080"
    volumes:
      - ./logs:/app/logs
    depends_on:
      - em-discovery
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081/actuator/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # API сервис
  em-api:
    build:
      context: ../em-api
      dockerfile: Dockerfile
    env_file:
      - ./env/api.prod.env
    expose:
      - "8080"                                                                                     # Работает внутри сети, порт публиковать не нужно
    volumes:
      - ./logs:/app/logs
    depends_on:
      - elasticsearch
      - em-discovery
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081/actuator/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Adapter сервис
  em-adapter:
    build:
      context: ../em-adapter
      dockerfile: Dockerfile
    env_file:
      - ./env/adapter.prod.env
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    expose:
      - "8080"
    volumes:
      - ./logs:/app/logs
    depends_on:
      - em-discovery
      - kafka
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081/actuator/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Collector сервис
  em-collector:
    build:
      context: ../em-collector
      dockerfile: Dockerfile
    env_file:
      - ./env/collector.prod.env
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    expose:
      - "8080"
    volumes:
      - ./logs:/app/logs
    depends_on:
      - em-discovery
      - kafka
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081/actuator/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Processor сервис
  em-processor:
    build:
      context: ../em-processor
      dockerfile: Dockerfile
    env_file:
      - ./env/processor.prod.env
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    expose:
      - "8080"
    volumes:
      - ./logs:/app/logs
    depends_on:
      - em-discovery
      - redis
      - kafka
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8081/actuator/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # =============================================================================

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.1
    env_file:
      - ./env/elasticsearch.prod.env
    environment:
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD:-secret}
      - ELASTIC_USERNAME=elastic
    ports:
      - "9200:9200"
      - "9300:9300"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - es_data:/usr/share/elasticsearch/data                                                       # Том для постоянного хранения данных
      - ./elasticsearch/elasticsearch.prod.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro # Монтирование кастомного файла конфигурации
      - ./elasticsearch/security.yml:/usr/share/elasticsearch/config/security.yml:ro                # Добавляем файл безопасности
      - ./elasticsearch/certs:/usr/share/elasticsearch/config/certs:ro                              # Монтируем директорию с сертификатами
    restart: always

  # Redis
  redis:
    image: redis:7.4.2
    restart: always
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data:rw                                                                 # Постоянное хранение данных Redis между перезапусками
      - ./redis/redis.prod.conf:/usr/local/etc/redis/redis.conf:ro                          # Подключаем защищённую конфигурацию Redis
    environment:
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis-password}
    command: [ "redis-server", "/usr/local/etc/redis/redis.conf" ]
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]                                                  # Проверка через ping
      interval: 30s                                                                         # Интервал проверки
      timeout: 10s                                                                          # Таймаут ожидания ответа
      retries: 3                                                                            # Количество попыток

  # Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:7.9.0
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000                                                             # Влияет на heartbeats и таймауты
      ZOOKEEPER_INIT_LIMIT: 5                                                               # Кол-во тиков, которое follower может тратить на начальную синхронизацию с leader
      ZOOKEEPER_SYNC_LIMIT: 2                                                               # Макс. кол-во тиков для синхронизации состояния между leader и follower

    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    healthcheck:
      test: ["CMD", "bash", "-c", "echo ruok | nc localhost 2181 | grep imok"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.9.0
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1                                                                    # Уникальный идентификатор брокера в кластере
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181                                               # Адрес Zookeeper для регистрации брокера
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092   # Список адресов, по которым брокер доступен снаружи и внутри контейнера
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT    # Протоколы для каждого listener
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT                                           # Listener, используемый для связи между брокерами
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1                                             # Репликация для топика со смещениями (offsets); 1 для single-broker окружения
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: true                                                 # Автоматическое создание топиков при обращении
      KAFKA_DELETE_TOPIC_ENABLE: true                                                       # Разрешение на удаление топиков
      KAFKA_LOG_RETENTION_HOURS: 72                                                         # Время хранения сообщений в часах
      KAFKA_NUM_PARTITIONS: 3                                                               # Кол-во партиций по умолчанию при создании топика
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1                                                   # Коэффициент репликации по умолчанию
      KAFKA_LOG_SEGMENT_BYTES: 1073741824                                                   # Размер сегмента логов (1 ГБ)
      KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: 300000                                         # Интервал проверки просроченных логов (в мс)
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always

  # Kafka Connect
  kafka-connect:
    build:
      context: ./kafka-connect                                                               # Путь к директории, где лежит Dockerfile
      dockerfile: Dockerfile
    ports:
      - "8383:8083" # REST API для управления коннекторами
    depends_on:
      - kafka
      - elasticsearch
    volumes:
      - ./kafka-connect/connectors:/etc/kafka-connect/connector-configs:ro                    # Монтируем наши JSON конфиги коннекторов
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "kafka:29092"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"                                      # Должно быть доступно другим сервисам в сети Docker
      CONNECT_REST_PORT: "8083"
      CONNECT_GROUP_ID: "event-mosaic-connect-prod"
      CONNECT_CONFIG_STORAGE_TOPIC: "_connect-configs-prod"
      CONNECT_OFFSET_STORAGE_TOPIC: "_connect-offsets-prod"
      CONNECT_STATUS_STORAGE_TOPIC: "_connect-status-prod"
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "1"                                          # Для prod среды с одним Kafka брокером это 1. Если брокеров больше, можно увеличить.
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "1"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"              # Путь к плагинам коннекторов
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: "All"
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8083/connectors || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: '1G'
        reservations:
          cpus: '0.5'
          memory: '512M'

  # Prometheus - система мониторинга и сбора метрик
  prometheus:
    image: prom/prometheus:v3.2.1
    env_file:
      - ./env/prometheus.prod.env
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'                                      # Путь к конфигурационному файлу
      - '--storage.tsdb.path=/prometheus'                                                   # Путь для хранения данных
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION_TIME:-15d}'                   # Время хранения данных (по умолчанию 15 дней)
      - '--web.console.libraries=/etc/prometheus/console_libraries'                         # Путь к js-библиотекам консоли
      - '--web.console.templates=/etc/prometheus/consoles'                                  # Путь к html-шаблонам консоли
      - '--web.enable-lifecycle'                                                            # Включение API жизненного цикла
      - '--web.enable-admin-api=${PROMETHEUS_WEB_ENABLE_ADMIN_API:-false}'                  # Включение/отключение админ API
    restart: always                                                                         # Всегда перезапускать контейнер
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy" ]          # Проверка здоровья сервиса
      interval: 30s                                                                         # Интервал проверки
      timeout: 10s                                                                          # Таймаут ожидания ответа
      retries: 3                                                                            # Количество попыток

  # Grafana - система визуализации метрик
  grafana:
    image: grafana/grafana:11.5.2
    env_file:
      - ./env/grafana.prod.env
    ports:
      - "3000:3000"
    volumes:
      - ./grafana/grafana.ini:/etc/grafana/grafana.ini:ro                                   # Монтируем кастомный конфиг файл
      - ./grafana/provisioning:/etc/grafana/provisioning:ro                                 # Монтируем директорию с настройками
      - grafana_data:/var/lib/grafana                                                       # Том для хранения данных
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}                                 # Имя пользователя из переменной окружения
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}                         # Пароль из переменной окружения
      - GF_USERS_ALLOW_SIGN_UP=false                                                        # Запрещаем регистрацию новых пользователей
      - GF_AUTH_ANONYMOUS_ENABLED=false                                                     # Отключаем анонимный доступ в продакшн
      - GF_FEATURE_TOGGLES_ENABLE=lokiLive,lokiLogRowContext                                # Включаем только функции для работы с логами
    depends_on:
      - prometheus                                                                          # Зависимость от Prometheus
      - loki                                                                                # Зависимость от Loki
    restart: always                                                                         # Всегда перезапускать контейнер
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Loki - система сбора и хранения логов
  loki:
    image: grafana/loki:3.4.2
    env_file:
      - ./env/loki.prod.env
    ports:
      - "3100:3100"                                                                         # Порт для API Loki
      - "7946:7946"                                                                         # Порт для memberlist (кластеризация)
    volumes:
      - ./loki/loki.prod.yml:/etc/loki/loki.prod.yml:ro                                     # Монтируем конфигурационный файл
      - loki_data:/loki                                                                     # Том для хранения данных
      - loki_wal:/wal                                                                       # Том для WAL директории
    command: >
      -config.file=${LOKI_CONFIG_PATH:-/etc/loki/loki.prod.yml}
      -log.level=${LOG_LEVEL:-info}
      -validation.allow-structured-metadata=false
    user: "0"                                                                               # Запускаем от имени root для решения проблем с правами
    depends_on:
      - minio                                                                               # Зависимость от MinIO
    restart: always                                                                         # Всегда перезапускать контейнер
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Promtail - агент для сбора логов
  promtail:
    image: grafana/promtail:3.4.2
    env_file:
      - ./env/promtail.prod.env
    volumes:
      - ./promtail/promtail.prod.yml:/etc/promtail/promtail.prod.yml:ro                     # Монтируем конфигурационный файл
      - ./logs:/logs:ro                                                                     # Доступ к логам микросервисов (только чтение)
      - /var/log:/var/log:ro                                                                # Доступ к логам хоста (только чтение)
      - /var/lib/docker/containers:/var/lib/docker/containers:ro                            # Доступ к логам контейнеров (только чтение)
      - promtail_data:/data                                                                 # Том для хранения позиций
    command: >
      -config.file=${PROMTAIL_CONFIG_PATH:-/etc/promtail/promtail.prod.yml}
      -log.level=${LOG_LEVEL:-info}
    depends_on:
      - loki                                                                                # Зависимость от Loki
    restart: always                                                                         # Всегда перезапускать контейнер

  # MinIO - S3-совместимое хранилище. Используется для хранения логов Loki
  minio:
    image: minio/minio:RELEASE.2025-02-28T09-55-16Z
    env_file:
      - ./env/minio.prod.env
    ports:
      - "9000:9000"                                                                         # Порт для API
      - "9001:9001"                                                                         # Порт для консоли
    volumes:
      - minio_data:/data                                                                    # Том для хранения данных
    command: server /data --console-address ":9001"                                         # Запуск сервера с консолью
    restart: always                                                                         # Всегда перезапускать контейнер
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3


volumes:
  es_data:                                                                                  # Том для хранения данных Elasticsearch
    driver: local
  redis_data:                                                                               # Том для хранения данных Redis
    driver: local
  prometheus_data:                                                                          # Том для хранения данных Prometheus
    driver: local
  grafana_data:                                                                             # Том для хранения данных Grafana
    driver: local
  minio_data:                                                                               # Том для хранения данных MinIO
    driver: local
  loki_data:                                                                                # Том для хранения данных Loki
    driver: local
  loki_wal:                                                                                 # Том для хранения WAL Loki
    driver: local
  promtail_data:                                                                            # Том для хранения данных Promtail
    driver: local
  kafka_data:                                                                               # Том для хранения данных Kafka
    driver: local
  zookeeper_data:                                                                           # Том для хранения данных Zookeeper
    driver: local
  zookeeper_log:                                                                            # Том для хранения логов Zookeeper
    driver: local